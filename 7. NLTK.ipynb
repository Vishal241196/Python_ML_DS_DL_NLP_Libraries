{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb4d670",
   "metadata": {},
   "source": [
    "# Natural Language Toolkit(NLTK):\n",
    "NLTK (Natural Language Toolkit) is a comprehensive platform for building Python programs to work with human language data. It's widely used in academia and industry for tasks ranging from simple text processing to advanced natural language understanding. \n",
    "\n",
    "# Features:\n",
    "\n",
    "### 1. Corpora: \n",
    "NLTK provides access to over 50 corpora and lexical resources, including WordNet, which is a large lexical database of English. These corpora cover a wide range of text genres and languages, facilitating research and development in NLP.\n",
    "\n",
    "### 2. Tokenization: \n",
    "NLTK offers tokenization tools for splitting text into words or sentences. This is a fundamental step in most NLP tasks, and NLTK provides various tokenizers to suit different needs.\n",
    "\n",
    "### 3. Stemming and Lemmatization: \n",
    "NLTK includes modules for stemming, which reduces words to their root form (e.g., \"running\" to \"run\"), and lemmatization, which converts words to their base or dictionary form (e.g., \"ran\" to \"run\"). These processes are essential for text normalization.\n",
    "\n",
    "### 4. Part-of-Speech (POS) Tagging: \n",
    "NLTK provides tools for tagging words in a text with their corresponding part-of-speech (e.g., noun, verb, adjective). POS tagging is crucial for many NLP tasks, such as syntactic parsing, information extraction, and sentiment analysis.\n",
    "\n",
    "### 5. Chunking and Parsing: \n",
    "NLTK supports chunking and parsing, which involve identifying syntactic structures in sentences. Chunking groups words into meaningful chunks (e.g., noun phrases, verb phrases), while parsing analyzes the grammatical structure of sentences.\n",
    "\n",
    "### 6. Named Entity Recognition (NER): \n",
    "NLTK includes tools for identifying and classifying named entities in text, such as names of people, organizations, locations, dates, and more. NER is essential for tasks like information extraction, entity linking, and question answering.\n",
    "\n",
    "### 7. Text Classification: \n",
    "NLTK supports text classification tasks, such as sentiment analysis, topic classification, and spam detection. It provides algorithms and utilities for feature extraction, model training, and evaluation.\n",
    "\n",
    "### 8. Language Models: \n",
    "NLTK allows you to build and train language models for tasks like language generation, machine translation, and spell checking. It includes tools for n-gram modeling, probabilistic context-free grammars (PCFGs), and more.\n",
    "\n",
    "### 9. WordNet Interface: \n",
    "NLTK provides an interface to WordNet, a lexical database of English. WordNet organizes words into synsets (sets of synonyms) and provides information about word meanings, relationships, and semantic similarity.\n",
    "\n",
    "### 10. Integration with Other Libraries: \n",
    "NLTK integrates with other Python libraries and tools, such as scikit-learn, TensorFlow, and spaCy, allowing you to combine its functionality with advanced machine learning and deep learning techniques.\n",
    "\n",
    "# Get it now\n",
    "- pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e82a5a",
   "metadata": {},
   "source": [
    "### Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e28381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in Brown Corpus: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
      "Synonyms of 'good': ['good.n.01', 'good.n.02', 'good.n.03', 'commodity.n.01', 'good.a.01', 'full.s.06', 'good.a.03', 'estimable.s.02', 'beneficial.s.01', 'good.s.06', 'good.s.07', 'adept.s.01', 'good.s.09', 'dear.s.02', 'dependable.s.04', 'good.s.12', 'good.s.13', 'effective.s.04', 'good.s.15', 'good.s.16', 'good.s.17', 'good.s.18', 'good.s.19', 'good.s.20', 'good.s.21', 'well.r.01', 'thoroughly.r.02']\n",
      "First 10 English stopwords: ['theirs', 'only', 'before', 'mightn', 'out', 'how', 'at', 'he', 'i', 'about']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')  # Tokenizers\n",
    "nltk.download('wordnet')  # WordNet\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "nltk.download('averaged_perceptron_tagger')  # POS Tagger\n",
    "nltk.download('maxent_ne_chunker')  # Named Entity Chunker\n",
    "nltk.download('words')  # Word List\n",
    "\n",
    "# Accessing a specific corpus - Brown Corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Access the words in the Brown Corpus\n",
    "brown_words = brown.words()\n",
    "\n",
    "# Print the first 20 words in the Brown Corpus\n",
    "print(\"First 20 words in Brown Corpus:\", brown_words[:20])\n",
    "\n",
    "# Accessing a specific corpus - WordNet\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Find synonyms of a word using WordNet\n",
    "synonyms = wordnet.synsets('good')\n",
    "\n",
    "# Print synonyms of the word 'good'\n",
    "print(\"Synonyms of 'good':\", [synonym.name() for synonym in synonyms])\n",
    "\n",
    "# Accessing a specific corpus - stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print the first 10 English stopwords\n",
    "print(\"First 10 English stopwords:\", list(stop_words)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16726ebd",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cefd20ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['NLTK', '(', 'Natural', 'Language', 'Toolkit', ')', 'is', 'a', 'comprehensive', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', \"'s\", 'widely', 'used', 'in', 'academia', 'and', 'industry', 'for', 'tasks', 'ranging', 'from', 'simple', 'text', 'processing', 'to', 'advanced', 'natural', 'language', 'understanding', '.', 'NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
      "Sentences: ['NLTK (Natural Language Toolkit) is a comprehensive platform for building Python programs to work with human language data.', \"It's widely used in academia and industry for tasks ranging from simple text processing to advanced natural language understanding.\", 'NLTK is a leading platform for building Python programs to work with human language data.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"NLTK (Natural Language Toolkit) is a comprehensive platform for building Python programs to work with human language data. It's widely used in academia and industry for tasks ranging from simple text processing to advanced natural language understanding. NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "words = word_tokenize(text)\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"Sentences:\", sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd0ff5d",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95363ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['NLTK', '(', 'Natural', 'Language', 'Toolkit', ')', 'comprehensive', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.', \"'s\", 'widely', 'used', 'academia', 'industry', 'tasks', 'ranging', 'simple', 'text', 'processing', 'advanced', 'natural', 'language', 'understanding', '.', 'NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac90ce",
   "metadata": {},
   "source": [
    "### Stemming / Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45a22633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['nltk', '(', 'natur', 'languag', 'toolkit', ')', 'is', 'a', 'comprehens', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'human', 'languag', 'data', '.', 'it', \"'s\", 'wide', 'use', 'in', 'academia', 'and', 'industri', 'for', 'task', 'rang', 'from', 'simpl', 'text', 'process', 'to', 'advanc', 'natur', 'languag', 'understand', '.', 'nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'human', 'languag', 'data', '.']\n",
      "Lemmatized Words: ['NLTK', '(', 'Natural', 'Language', 'Toolkit', ')', 'is', 'a', 'comprehensive', 'platform', 'for', 'building', 'Python', 'program', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', \"'s\", 'widely', 'used', 'in', 'academia', 'and', 'industry', 'for', 'task', 'ranging', 'from', 'simple', 'text', 'processing', 'to', 'advanced', 'natural', 'language', 'understanding', '.', 'NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'program', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(word) for word in words]\n",
    "\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ba14c",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "680f4be7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f8c13d36d125>:12: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  dep_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-parser\\.jar jar file at /path/to/stanford-parser.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f8c13d36d125>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Create Stanford Dependency Parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdep_parser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStanfordDependencyParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_jar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_to_jar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_models_jar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_to_models_jar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Perform dependency parsing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    399\u001b[0m         )\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# find the most recent code and model jar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         stanford_jar = max(\n\u001b[0m\u001b[0;32m     51\u001b[0m             find_jar_iter(\n\u001b[0;32m     52\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    717\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m             raise LookupError(\n\u001b[0m\u001b[0;32m    720\u001b[0m                 \u001b[1;34mf\"Could not find {name_pattern} jar file at {path_to_jar}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m             )\n",
      "\u001b[1;31mLookupError\u001b[0m: Could not find stanford-parser\\.jar jar file at /path/to/stanford-parser.jar"
     ]
    }
   ],
   "source": [
    "from nltk.parse.dependencygraph import DependencyGraph\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Path to Stanford Parser (make sure to download and set it up)\n",
    "path_to_jar = '/path/to/stanford-parser.jar'\n",
    "path_to_models_jar = '/path/to/stanford-parser-3.9.2-models.jar'\n",
    "\n",
    "# Create Stanford Dependency Parser\n",
    "dep_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "# Perform dependency parsing\n",
    "result = dep_parser.raw_parse(sentence)\n",
    "\n",
    "# Get first parsed result (there could be multiple depending on the sentence complexity)\n",
    "dep_tree = next(result)\n",
    "\n",
    "# Print dependency tree\n",
    "print(dep_tree.to_dot())\n",
    "\n",
    "# Alternatively, you can print the list of tuples representing dependencies\n",
    "# for triple in dep_tree.triples():\n",
    "#     print(triple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b853e",
   "metadata": {},
   "source": [
    "### Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ae6177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('NLTK', 'NNP'), ('(', '('), ('Natural', 'NNP'), ('Language', 'NNP'), ('Toolkit', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('comprehensive', 'JJ'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('widely', 'RB'), ('used', 'VBN'), ('in', 'IN'), ('academia', 'NN'), ('and', 'CC'), ('industry', 'NN'), ('for', 'IN'), ('tasks', 'NNS'), ('ranging', 'VBG'), ('from', 'IN'), ('simple', 'JJ'), ('text', 'NN'), ('processing', 'NN'), ('to', 'TO'), ('advanced', 'VB'), ('natural', 'JJ'), ('language', 'NN'), ('understanding', 'NN'), ('.', '.'), ('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5cca32",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c7a7a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Tags: (S\n",
      "  (GPE NLTK/NNP)\n",
      "  (/(\n",
      "  (ORGANIZATION Natural/NNP Language/NNP Toolkit/NNP)\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  comprehensive/JJ\n",
      "  platform/NN\n",
      "  for/IN\n",
      "  building/VBG\n",
      "  (PERSON Python/NNP)\n",
      "  programs/NNS\n",
      "  to/TO\n",
      "  work/VB\n",
      "  with/IN\n",
      "  human/JJ\n",
      "  language/NN\n",
      "  data/NNS\n",
      "  ./.\n",
      "  It/PRP\n",
      "  's/VBZ\n",
      "  widely/RB\n",
      "  used/VBN\n",
      "  in/IN\n",
      "  academia/NN\n",
      "  and/CC\n",
      "  industry/NN\n",
      "  for/IN\n",
      "  tasks/NNS\n",
      "  ranging/VBG\n",
      "  from/IN\n",
      "  simple/JJ\n",
      "  text/NN\n",
      "  processing/NN\n",
      "  to/TO\n",
      "  advanced/VB\n",
      "  natural/JJ\n",
      "  language/NN\n",
      "  understanding/NN\n",
      "  ./.\n",
      "  (ORGANIZATION NLTK/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  leading/VBG\n",
      "  platform/NN\n",
      "  for/IN\n",
      "  building/VBG\n",
      "  (PERSON Python/NNP)\n",
      "  programs/NNS\n",
      "  to/TO\n",
      "  work/VB\n",
      "  with/IN\n",
      "  human/JJ\n",
      "  language/NN\n",
      "  data/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "ner_tags = ne_chunk(pos_tags)\n",
    "\n",
    "print(\"NER Tags:\", ner_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1260c3b9",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6acc44c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT big/JJ cat/NN)\n",
      "  (VP sat/VBD)\n",
      "  (PP on/IN (NP the/DT mat/NN)))\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "# Example sentence\n",
    "sentence = [(\"the\", \"DT\"), (\"big\", \"JJ\"), (\"cat\", \"NN\"), (\"sat\", \"VBD\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "\n",
    "# Define chunk grammar\n",
    "chunk_grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>} # Chunk NP: optional determiner, followed by any number of adjectives, followed by a noun\n",
    "    VP: {<VB.*><NP|PP>*} # Chunk VP: verb followed by NP or PP\n",
    "    PP: {<IN><NP>} # Chunk PP: preposition followed by NP\n",
    "\"\"\"\n",
    "\n",
    "# Create chunk parser\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# Perform chunking\n",
    "chunked_sentence = chunk_parser.parse(sentence)\n",
    "\n",
    "# Print chunked sentence\n",
    "print(chunked_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c437008",
   "metadata": {},
   "source": [
    "### Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1510d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequency: [('for', 3), ('to', 3), ('language', 3), ('.', 3), ('NLTK', 2)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(words)\n",
    "\n",
    "print(\"Word Frequency:\", fdist.most_common(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e3e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
